import os
import json
import asyncio
import logging
import numpy as np
import faiss
from dotenv import load_dotenv
from telegram import Update
from telegram.ext import ApplicationBuilder, CommandHandler, ContextTypes
from google.cloud import aiplatform
import google.generativeai as genai

# =====================================
# üîß Load environment variables
# =====================================
load_dotenv()
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
PROJECT_ID = os.getenv("GOOGLE_PROJECT_ID")
LOCATION = os.getenv("GOOGLE_LOCATION", "us-central1")

EMBED_MODEL = "text-embedding-005"
GEN_MODEL = "gemini-2.5-flash"

aiplatform.init(project=PROJECT_ID, location=LOCATION)
genai.configure(project=PROJECT_ID, location=LOCATION)

logging.basicConfig(level=logging.INFO)

# =====================================
# üìÑ Initialize sample documents
# =====================================
DOCS = {
    "policy.txt": "Employees must submit leave requests 2 days in advance. Remote work is allowed up to 3 days a week.",
    "recipe.txt": "To make pasta: boil water, add salt, cook pasta for 8-10 minutes, then drain and mix with sauce.",
    "faq.txt": "To reset your password, go to settings, click 'Reset Password', and follow the email instructions.",
    "personal.txt": "My name is Susu and age is 105. Eats chicken and favourite colour is Red.",
}

os.makedirs("data", exist_ok=True)
DOC_PATH = "data/docs.json"

if not os.path.exists(DOC_PATH):
    with open(DOC_PATH, "w") as f:
        json.dump(DOCS, f)

# =====================================
# üß† Memory and Cache
# =====================================
USER_HISTORY = {}   # {user_id: [{"q": query, "a": answer}]}
EMBED_CACHE = {}    # {text: np.ndarray}

# =====================================
# üîç Embedding + FAISS Index
# =====================================
def get_text_embedding(text: str):
    """Generate embeddings and cache results"""
    if text in EMBED_CACHE:
        return EMBED_CACHE[text]
    from vertexai.preview.language_models import TextEmbeddingModel
    model = TextEmbeddingModel.from_pretrained(EMBED_MODEL)
    emb = np.array(model.get_embeddings([text])[0].values, dtype="float32")
    EMBED_CACHE[text] = emb
    return emb

def build_faiss_index():
    """Build FAISS index for stored docs"""
    with open(DOC_PATH) as f:
        docs = json.load(f)
    keys, vectors = [], []
    for k, v in docs.items():
        keys.append(k)
        vectors.append(get_text_embedding(v))
    vectors = np.vstack(vectors)
    index = faiss.IndexFlatL2(vectors.shape[1])
    index.add(vectors)
    faiss.write_index(index, "data/index.faiss")
    with open("data/keys.json", "w") as f:
        json.dump(keys, f)
    logging.info("‚úÖ FAISS index built successfully.")

def retrieve_similar(query, top_k=1):
    """Return best matching doc snippet"""
    index = faiss.read_index("data/index.faiss")
    with open("data/keys.json") as f:
        keys = json.load(f)
    qvec = get_text_embedding(query).reshape(1, -1)
    D, I = index.search(qvec, top_k)
    best_key = keys[I[0][0]]
    with open(DOC_PATH) as f:
        docs = json.load(f)
    return docs[best_key], best_key

if not os.path.exists("data/index.faiss"):
    build_faiss_index()

# =====================================
# ü§ñ Gemini Generator
# =====================================
async def generate_answer(context_text, user_query, user_history=None):
    """Generate context-aware response"""
    hist_text = ""
    if user_history:
        for h in user_history[-3:]:
            hist_text += f"User: {h['q']}\nBot: {h['a']}\n"

    prompt = f"""
You are a helpful assistant. Use the context below to answer.

Context:
{context_text}

Conversation history:
{hist_text}

User query:
{user_query}

Answer clearly and concisely.
"""
    model = genai.GenerativeModel(GEN_MODEL)
    response = model.generate_content(prompt)
    return response.text.strip()

# =====================================
# üßæ Telegram Commands
# =====================================
async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "üí° *Commands:*\n"
        "/ask <query> ‚Üí Ask about docs\n"
        "/summarize ‚Üí Summarize recent chat",
        parse_mode="Markdown"
    )

async def ask(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.message.from_user.id
    query = " ".join(context.args)
    if not query:
        await update.message.reply_text("Please provide a query. Example: /ask What is the leave policy?")
        return

    context_text, source = retrieve_similar(query)
    history = USER_HISTORY.get(user_id, [])

    try:
        answer = await generate_answer(context_text, query, history)
    except Exception as e:
        logging.error(f"Gemini error: {e}")
        await update.message.reply_text(f"‚ö†Ô∏è Error generating answer: {e}")
        return

    USER_HISTORY.setdefault(user_id, []).append({"q": query, "a": answer})
    USER_HISTORY[user_id] = USER_HISTORY[user_id][-3:]

    await update.message.reply_text(
        f"üß† *Answer:*\n{answer}\n\nüìÑ *Source:* `{source}`",
        parse_mode="Markdown"
    )

async def summarize(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Summarize recent chat"""
    user_id = update.message.from_user.id
    history = USER_HISTORY.get(user_id, [])
    if not history:
        await update.message.reply_text("No recent chat to summarize.")
        return

    chat_text = "\n".join([f"User: {h['q']}\nBot: {h['a']}" for h in history])
    prompt = f"Summarize briefly:\n\n{chat_text}"

    model = genai.GenerativeModel(GEN_MODEL)
    response = model.generate_content(prompt)
    summary = response.text.strip()

    await update.message.reply_text(f"üìù *Summary:*\n{summary}", parse_mode="Markdown")

# =====================================
# üöÄ Main Entrypoint
# =====================================
async def main():
    app = ApplicationBuilder().token(TELEGRAM_TOKEN).build()
    app.add_handler(CommandHandler("help", help_command))
    app.add_handler(CommandHandler("ask", ask))
    app.add_handler(CommandHandler("summarize", summarize))
    logging.info("üöÄ Bot started‚Ä¶")
    await app.run_polling()

if __name__ == "__main__":
    asyncio.run(main())
